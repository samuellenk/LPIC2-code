---
title: "Praxis-Beispiel: RAID konfigurieren ![](images/LPIC2_logo.png){width=20 height=20}"
subtitle: "LPIC 2"
author: "![SL](images/SL_foto_300.png){width=20 height=20} [&copy; Samuel Lenk](https://linux-trainings.de/)"
theme: "Luebeck"
colortheme: "whale"
aspectratio: 169
colorlinks: true
urlcolor: gray
linkcolor: gray
---

# Voraussetzungen

- für das Nachvollziehen der Beispiele bietet sich eine VM an, der man mehrere Platten hinzufügen kann
- für RAID-5 brauchst du mindestens vier Festplatten

# Vorbereitung

Installation vom Paket für RAID:
```bash
sudo apt install mdadm
```

Kernel-Modul geladen:
```bash
lsmod | grep -E '(raid|md_mod)'
```

# Übersicht erhalten

Features ansehen:
```bash
cat /proc/mdstat
```

Beispiel-Ausgabe:
```bash
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]
unused devices: <none>
```

- die angezeigten `Personalities` hängen von den aktuell geladenen Kernel-Modulen ab
- und die nötigen Kernel-Module werden automatisch geladen, sobald ein RAID-Array erstellt wird

# RAID-Array erstellen

verwendbare Platten anzeigen:
```bash
lsblk
```

- spätestens jetzt vier Platten im Hypervisor hinzufügen

Beispiel-Ausgabe:
```bash
NAME                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
...
vdb                   254:16   0    1G  0 disk
vdc                   254:32   0    1G  0 disk
vdd                   254:48   0    1G  0 disk
vde                   254:64   0    1G  0 disk
```

RAID-5-System mit drei Geräten anlegen:
```bash
sudo mdadm --create \
  --verbose /dev/md0 \
  --level=raid5 \
  --raid-devices=3 /dev/vdb /dev/vdc /dev/vdd
```

Aufbau ansehen bzw. Ergebnis betrachten:
```bash
cat /proc/mdstat
```

Beispiel-Ausgabe:
```bash
Personalities : [linear] [raid6] [raid5] [raid4]
md0 : active raid5 vdd[3] vdc[1] vdb[0]
      2095104 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]
      [>....................] recovery = 3.8% (40320/1047552) \
      finish=2.9min speed=5760K/sec
```

enthaltene Einträge:
```bash
[3/2] # 2 von 3 Platten sind bereit
[UU_] # U = Up, _ = Down
```

# RAID-Array verwenden

Dateisystem erzeugen:
```bash
sudo mkfs.ext4 /dev/md0
```

Mount-Point anlegen:
```bash
sudo mkdir /mnt/raid
```

Mount ausführen:
```bash
sudo mount -t ext4 /dev/md0 /mnt/raid
```

- bitte beachte, dass die Konfiguration noch persistiert werden muss, damit das Array auch nach einem Neustart wieder da ist

# RAID-Array persistieren

Eintrag in `/etc/mdadm/mdadm.conf` schreiben:
```bash
sudo mdadm --examine --scan | sudo tee -a /etc/mdadm/mdadm.conf
```

Ergebnis überprüfen:
```bash
cat /etc/mdadm/mdadm.conf
```

Achtung bei Device-Namen:

- unter Ubuntu/Debian ändern sich diese bei jedem Start
- kann immer unter `/proc/mdstat` überprüft werden
- daher ist natürlich ein Eintrag in `/etc/fstab` sinnvoll, um einerseits die UUID zu verwenden und andererseits direkt immer wieder den Mount zu erhalten

# Ersatz-Platte hinzufügen

- zunächst neue Platte im Hypervisor hinzufügen

dann Spare Disk/Hot Spare einbinden:
```bash
sudo mdadm --add /dev/md0 /dev/vde
```

Ergebnis prüfen:
```bash
cat /proc/mdstat
```

- das `(S)` hinter dem Namen der Platte zeigt an, dass es sich um einen **Spare** Disk handelt

# fehlerhafte Platte austauschen

Fehler simulieren:
```bash
sudo mdadm --manage /dev/md0 --fail /dev/vdd
```

- das `(F)` hinter dem Namen der Platte zeigt an, dass es sich um eine **Failed** Disk handelt

- bei neuen Versionen von `mdadm` werden fehlerhafte Platten automatisch aus dem Verbund entfernt

andernfalls Platte manuell austauschen:
```bash
sudo mdadm --manage /dev/md0 --replace /dev/vdd
```

- wenn es dann aber zu wenige Platten gibt, bleibt das Array stehen und man muss selbst Hand anlegen

# RAID-Array neu aufbauen

lassen wir noch eine Platte versagen:
```bash
sudo mdadm --manage /dev/md0 --fail /dev/vdc
```

aushängen:
```bash
sudo umount /mnt/raid
```

Array zunächst stoppen:
```bash
sudo mdadm --manage --stop /dev/md0
cat /proc/mdstat # unused devices sollte jetzt leer sein
```

Analyse der Festplatten:
```bash
sudo mdadm --misc --examine /dev/vdb
# gleiche Aktion für die anderen beteiligten Festplatten wieder holen
```

- bei allen Platten muss die Array-UUID identisch sein
- wir hatten ein Array aus drei Platten gebaut und einen Spare hinzugefügt
- also sollten wir ohne Datenverlust wieder aufbauen können

RAID-Array neu aufbauen:
```bash
sudo mdadm --assemble --run /dev/md0 /dev/vdc /dev/vdd /dev/vde
```

Ergebnis prüfen:
```bash
cat /proc/mdstat
```

sicherheitshalber eine Datensystemprüfung ausführen:
```bash
sudo fsck /dev/md0
```

# RAID-Array vergrössern

vorhandene Spare-Disk aktiv nutzen lassen:
```bash
sudo mdadm --grow /dev/md0 --raid-devices=4
```

Ergebnis prüfen:
```bash
cat /proc/mdstat
```

- das Umziehen der Daten wird als `Reshape` angezeigt
- jetzt warten bis das fertig ist, weil das eine gute Weile dauern kann

zum Abschluss das Dateisystem vergrössern:
```bash
sudo resize2fs /dev/md0
```

Mount ausführen:
```bash
sudo mount /dev/md0 /mnt/raid
```

neue Grösse prüfen:
```bash
df -h /mnt/raid
```

# RAID-Array entfernen

aushängen:
```bash
sudo umount /mnt/raid
```

Array entfernen aus `/etc/mdadm/mdadm.conf`:
```bash
sudo mdadm --stop /dev/md0
```

RAID Superblocks entfernen:
```bash
for DISK in {b..f}; do
  sudo mdadm --zero-superblock /dev/vd$DISK
done
```

optional die Platten überschreiben:
```bash
# so werden die Platten vdb - vdf überschrieben:
for DISK in {b..f}; do
  sudo dd if=/dev/zero bs=4096 of=/dev/vd$DISK
done
```

- jetzt können die Platten wieder anderweitig verwendet werden
